# LLM from scratch
### Transforer Architecture
---
**B = batch size   the no of sequences(sentences) we process in parallel**

**T = Time (sequence length / context length) Number of tokens in each sequence.  if T = 8, each input has 8 tokens (like 8 words/chars).**

**V = vocab size (number of possible tokens)**

**C = Dimensionality of the vector representing each token. Example: if C = 384, each token is mapped to a 384-dimensional embedding vector.**

**The input to a Transformer is usually shaped:**
**(B, T, C)**


---

# 📝 Encoder Block – Cheatsheet

### Self Attention

* In **self attention** all 3 vectors (Query, Key, Value) come from the **same sequence**.
* In **multi-head attention** we have multiple self-attention layers (heads) inside the block to capture **different meanings** of the input sequence.
* In **cross attention** the Query comes from one sequence (decoder output) while Key and Value come from another sequence (encoder output).

---

### How Q, K, V are made

* Query, Key, and Value vectors are generated by applying **linear transformation** on the embedding vector.
* Usually done with a small neural network (linear layers) that learns the projection matrices.
* Then dot product is applied to form attention scores.

---

### Flow of Encoder Block

1. **Tokenize input**

   * Convert words → tokens → numbers.

2. **Embedding vector**

   * Convert token IDs → dense embedding vectors.

3. **Positional encoding**

   * Create positional encoding (same dimension as embedding).
   * Add embedding + positional encoding → gives input with position info.

4. **Self Attention layer**

   * Generate Q, K, V vectors using linear transformation.
   * Compute dot product: ( w = Q \times K^T ).
   * Normalize: divide by (\sqrt{\text{dimension of K}}).
   * Apply softmax → get normalized weights in range 0–1.
   * Multiply with Value vector: ( y = w \times V ).

5. **Feed Forward Neural Network (FFN)**

   * Pass the result into a neural network.
   * Apply **ReLU activation** to add non-linearity.
   * Output layer of the FFN has same dimension as embedding vector.

6. **Final Output**

   * Encoder output = **contextual embedding** of each token.
---

# 📝 Decoder Block – Cheatsheet

### Cross vs Self Attention

* **Decoder has 2 attention layers**:

  1. **Masked Self Attention** → lets decoder look at past tokens only no futer token done by ==> wei.masked_fill(tril==0,float("-inf")) .
  2. **Cross Attention** → Query comes from decoder, Key + Value come from encoder output.

---

### Flow of Decoder Block

1. **Tokenize target sequence**

   * Convert target sentence (shifted right during training to add <SOS>) → tokens → numbers.

2. **Embedding vector**

   * Convert token IDs → dense embedding vectors.

3. **Positional encoding**

   * Add positional encoding to embeddings(same dimension as input embedding).

4. **Masked Self Attention**

   * Generate Q, K, V from decoder input (like encoder).
   * Apply mask (future tokens hidden) ==> wei.masked_fill(tril==0,float("-inf")).
   * Compute attention: ( w = softmax(Q.K.T/sqrt(Dimesnion of K))) . V
   * This ensures each position only attends to **previous tokens**.

5. **Cross Attention**

   * Query (Q) from decoder hidden states.
   * Key (K), Value (V) from encoder output.
   * Attention = how decoder tokens attend to encoder tokens.
   * Output is contextual info combining encoder + decoder states.

6. **Feed Forward Neural Network (FFN)**

   * Pass attention output into FFN with ReLU for non-linearity.
   * Output dimension = same as embedding size.

7. **Final Softmax Layer (Prediction)**

   * After stacking decoder blocks, final layer projects output → vocabulary size.
   * Apply softmax → gives probability distribution of next token.

---
